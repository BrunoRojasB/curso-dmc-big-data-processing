<h1>Apache Spark</h1>
<ol>
<li>Laboratorio Desarrollo en Notebook con Apache Spark</li>
	<ul>
		<li>Jupyter Notebooks sobre Apache Spark en Google Cloud Platform</li>
		<li>Importación de Módulos</li>
		<li>Crear Sesión de Spark</li>
		<li>El Spark Console crea la sesión por defecto. </li>
		<li>Dataframes con Schema</li>
		<li>Creación de bucket para Cloud Storage en Google Cloud Plataform</li>
		<li>Creamos un bucket y una carpet dentro del bucket</li>
		<li>Carga de archivo a la carpeta del bucket</li>
		<li>Almacenar en un dataframe la lectura de archivos externos con spark.read.format</li>
		<li>Función show para mostrar datos de un dataframe</li>
		<li>Tipos StrucType y StructField para definir esquemas</li>
		<li>PrintSchema() para ver el esquema del dataframe.</li>
		<li>Función Select </li>
		<li>Función withColumn, col, cast, when, otherwise</li>
		<li>Función withColumnRenamed, drop, lit</li>
		<li>Función Filter e isin</li>
		<li>Función like</li>
		<li>Función count, distinct, dropDuplicates</li>
		<li>Función orderBy, asc, desc, groupBy, agg, sum, avg, max</li>
		<li>Función where, partitionby, write.mode, format, save</li>
	</ul>
<li>Apache Spark en Databricks Community</li>
	<ul>
		<li>Creación de Credenciales a través de GCP-IAM & Admin->Service Accounts</li>
		<li>Seteamos las credenciales en la configuración del cluster en databricks</li>
		<li>Conectamos databricks con nuestro bucket de gcp</li>
		<li>Ejercicios con data de kaggle</li>
	</ul>
</ol>
