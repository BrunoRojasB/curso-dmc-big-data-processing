# curso-dmc-big-data-processing
Talleres Desarrollados Durante el Curso de Big Data DMC

<h1>
  Índice de temas DataProc
</h1>
<ul>
  <li>Creación de Clúster Dataproc en Google Cloud Platform</li>
  <li>Creación de llaves SSH para conexión con Cluster</li>
  <li>Convertir IP Efímera en IP Estática</li>
  <li>Instalación y Configuración de Remote Explorer en Visual Studio Code</li>
</ul>

<h1>
  Índice de Temas HDFS
</h1>
<ul>
  <li>Creación de un Cluster en Google Cloud Platform con Dataproc</li>
  <li>Instalación de Remote Explorer en Visual Studio Code</li>
  <li>Generación de llaves públicas y privadas para conexión a Cluster vía SSH</li>
  <li>Conexión al Cluster de Google Cloud a través de Remote Explorer en Visual Studio Code</li>
  <li>Comandos básicos de HDF: Comando HDFS DFS </li>
  <li>Listar directorios: Comado ls, ls -R</li>
  <li>Crear Carpetas: Comandos mkdir, rm</li>
  <li>Copiar archivos de Linux a HDFS: Comando Put</li>
  <li>Asignación de permisos de ejecución, escritura y lectura: Comando Chmod</li>
  <li>Cambio de dueño: Comando Chown</li>
  <li>Cambiando a superusuario de Linux: sudo su</li>
  <li>Cambiando a superusuario de hdfs: su hdfs</li>
  <li>Cambiando dueño de carpeta: hdfs dfs -chown dmc:bigdata /bruno_rojas/carpeta2</li>
  <li>Cambiando dueño de carpeta de forma recursiva: hdfs dfs -chown -R dmc:bigdata /bruno_rojas/carpeta2</li>
  <li>Permisos con ACL Comando setfacl: hdfs dfs -setfacl -R -m user:nombreusuario:wwx /ruta</li>
  <li>Verificación de la integridad de los datos cksum /home/dev_r/persona.data</li>
  <li>Cambiando el número de replicas a 3 Comando hdfs dfs -setrep -w 3 -R /brunorojas</li>
</ul>

